{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pylamp.loss.losses import CrossEntropyLoss\n",
    "from pylamp.neural.sequential import Sequential\n",
    "from pylamp.neural.layers import Linear, Conv1D, MaxPool1D, Flatten\n",
    "from pylamp.optim.optimizer import Optim\n",
    "from pylamp.neural.activations import ReLU\n",
    "from pylamp.optim.optimizer import SGD\n",
    "from pylamp.utils.usps import load_usps\n",
    "from pylamp.utils.plotter import Display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "uspsdatatrain = \"./data/USPS/USPS_train.txt\"\n",
    "uspsdatatest = \"./data/USPS/USPS_test.txt\"\n",
    "X_train, y_train = load_usps(uspsdatatrain)\n",
    "X_test, y_test = load_usps(uspsdatatest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shape-mismatch for sum",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[103], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m CrossEntropyLoss()\n\u001b[0;32m     10\u001b[0m optimizer_model \u001b[38;5;241m=\u001b[39m Optim(model, loss, \u001b[38;5;241m1e-3\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m model_losses \u001b[38;5;241m=\u001b[39m \u001b[43mSGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_channel_x\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_channel_y\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Cours-Sorbonne\\S2\\ML\\PROJET\\projet\\src\\pylamp\\optim\\optimizer.py:65\u001b[0m, in \u001b[0;36mSGD\u001b[1;34m(network, X_train, y_train, batch_size, epochs, verbose, add_channel_x, add_channel_y)\u001b[0m\n\u001b[0;32m     61\u001b[0m train_data \u001b[38;5;241m=\u001b[39m dg\u001b[38;5;241m.\u001b[39mbatch_generator(\n\u001b[0;32m     62\u001b[0m     X_train_shuffled, y_train_shuffled, batch_size, add_channel_x, add_channel_y)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_x, batch_y \u001b[38;5;129;01min\u001b[39;00m train_data:\n\u001b[1;32m---> 65\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m num_batches\n\u001b[0;32m     68\u001b[0m train_loss_tracker\u001b[38;5;241m.\u001b[39mappend(avg_loss)\n",
      "File \u001b[1;32mc:\\Cours-Sorbonne\\S2\\ML\\PROJET\\projet\\src\\pylamp\\optim\\optimizer.py:24\u001b[0m, in \u001b[0;36mOptim.step\u001b[1;34m(self, batch_x, batch_y)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m     26\u001b[0m loss_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mforward(batch_y, output)\n",
      "File \u001b[1;32mc:\\Cours-Sorbonne\\S2\\ML\\PROJET\\projet\\src\\pylamp\\neural\\sequential.py:33\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     30\u001b[0m inputs \u001b[38;5;241m=\u001b[39m [X]\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules:\n\u001b[1;32m---> 33\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     inputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_inputs \u001b[38;5;241m=\u001b[39m inputs\n",
      "File \u001b[1;32mc:\\Cours-Sorbonne\\S2\\ML\\PROJET\\projet\\src\\pylamp\\neural\\layers.py:83\u001b[0m, in \u001b[0;36mConv1D.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     76\u001b[0m windows_view \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlib\u001b[38;5;241m.\u001b[39mstride_tricks\u001b[38;5;241m.\u001b[39mas_strided(\n\u001b[0;32m     77\u001b[0m     X,\n\u001b[0;32m     78\u001b[0m     shape\u001b[38;5;241m=\u001b[39m(batch, output_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size, in_channel),\n\u001b[0;32m     79\u001b[0m     strides\u001b[38;5;241m=\u001b[39m(X\u001b[38;5;241m.\u001b[39mstrides[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrides \u001b[38;5;241m*\u001b[39m X\u001b[38;5;241m.\u001b[39mstrides[\u001b[38;5;241m1\u001b[39m], X\u001b[38;5;241m.\u001b[39mstrides[\u001b[38;5;241m1\u001b[39m], X\u001b[38;5;241m.\u001b[39mstrides[\u001b[38;5;241m2\u001b[39m]),\n\u001b[0;32m     80\u001b[0m     writeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     81\u001b[0m )\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Calcul de la convolution en une seule opÃ©ration de multiplication matricielle\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensordot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindows_view\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweights\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\halim\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\numeric.py:1099\u001b[0m, in \u001b[0;36mtensordot\u001b[1;34m(a, b, axes)\u001b[0m\n\u001b[0;32m   1097\u001b[0m             axes_b[k] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ndb\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m equal:\n\u001b[1;32m-> 1099\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape-mismatch for sum\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;66;03m# Move the axes to sum over to the end of \"a\"\u001b[39;00m\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;66;03m# and to the front of \"b\"\u001b[39;00m\n\u001b[0;32m   1103\u001b[0m notin \u001b[38;5;241m=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nda) \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m axes_a]\n",
      "\u001b[1;31mValueError\u001b[0m: shape-mismatch for sum"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add_module(Conv1D(kernel_size=3, in_channels=1, out_channels=32, strides=1))\n",
    "model.add_module(MaxPool1D(kernel_size=2, strides=2))\n",
    "model.add_module(Flatten())\n",
    "model.add_module(Linear(4064,100))\n",
    "model.add_module(ReLU())\n",
    "model.add_module(Linear(100,10))\n",
    "\n",
    "loss = CrossEntropyLoss()\n",
    "optimizer_model = Optim(model, loss, 1e-3)\n",
    "model_losses = SGD(optimizer_model, X_train, y_train, batch_size=256, epochs=100, verbose=True, add_channel_x=True, add_channel_y=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Display.plot_loss(model_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
